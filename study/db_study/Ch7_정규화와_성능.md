# 공부한 내용


## 정규화에 대한 사전 학습

### 제 1 정규화

하나의 속성에는 하나의 값만 넣는다. 예를 들어 헬스장 회원이 있을 때

| 회원번호 | 회원이름 | 프로그램 |
| --- | --- | --- |
| 1 | 김민석 | 헬스, 골프 |

| 회원번호 | 회원이름 | 프로그램 |
| --- | --- | --- |
| 1 | 김민석 | 헬스 |
| 1 | 김민석 | 골프 |

왼쪽과 같이 한 번에 2개의 값을 넣지 않고 오른쪽처럼 나눠서 넣어야 한다.

이유는 간단하다. 쿼리로 조회를 할 때 WHERE=’헬스’ 이렇게 찾을 수 있는 데이터를 LIKE=’%헬스%’ 이렇게 찾아야 되서 성능 저하가 발생한다.

### 제 2 정규화

현재 테이블의 주제와 크게 관련이 없는 컬럼(속성)을 다른 테이블로 빼는 작업이다.

| 회원번호 | 회원이름 | 프로그램 | 가격 |
| --- | --- | --- | --- |
| 1 | 김민석 | 헬스 | 4000 |
| 2 | 김민수 | 골프 | 5000 |
| 3 | 김민서 | 골프 | 5000 |

위와 같은 평범한 테이블에서 골프의 가격을 바꾼다고 하면, 2개의 행만 업데이트해주면 된다.

하지만, 만약 회원이 10만명 정도 있다고 가정하면 업데이트에 드는 리소스가 굉장히 증가하게 된다.

| 프로그램번호 | 프로그램이름 | 가격 |
| --- | --- | --- |
| 1 | 헬스 | 4000 |
| 2 | 골프 | 5000 |

그럴 때는 이와 같이 주제와 상관없는 컬럼을 따로 빼서 테이블로 만들면 된다. 위 테이블에서 가격을 한 번만 바꾸면 해당 가격을 참조한 나머지 테이블의 수많은 행에 저절로 적용이 되기 때문이다.

좀더 자세히 설명하자면, partial dependency를 제거하면 된다. 가격과 같이 오직 프로그램 이름이라는 하나의 컬럼에 의존하는 값이 있고, 그것이 Composite Primary Key(부분키)에 속한다면 그것을 제거하면 된다.

하지만 이 방법도 단점이 있는데, 회원이름과 프로그램의 가격이 각기 다른 테이블에 있기 때문에 같이 조회하려면 무조건 JOIN 연산을 해야 한다는 단점이 있다.

### 제 3 정규화

제 2 정규화와 비슷하나, Composite Primary Key에 속하는 컬럼이 아닌, 일반 컬럼에 종속된 컬럼을 제거하는 형태이다.

### 결론?

정규화라는 것은 컬럼의 수정을 편리하게 하기 위한 작업이다. 원래대로라면 수만, 수십만 건의 데이터를 일일히 수정해야하는 작업인데, 정규화를 통해 테이블을 분리함으로써 비교적 아주 적은 비용으로 해당 작업을 수행할 수 있다.

하지만, 결국에 테이블을 쪼개면 쪼갤수록 조회할 때 JOIN이 필요하기 때문에 그에 대는 비용 역시 증가하게 된다. 어쨌든 Trade-Off가 있기는 하지만, 일반적으로 제 3정규화까지는 하는 게 효율이 잘 나오기 때문에 데이터 모델링 시에는 제 3정규화까지 고려해봐야 한다.

## 1. 정규화를 통한 성능 향상 전략

일반적으로 정규화를 진행하면 관심사 및 테이블 분리로 인해 입력/수정/삭제 에 대한 성능은 상승하지만 조회에 대한 성능은 떨어지는 경우가 많다.

조회에서 성능이 떨어지는 이유는 간단하다. 테이블을 많이 분리했기 때문에, 이로 인한 조회에서의 JOIN 연산 비용이 추가적으로 생김으로써 성능이 떨어지게 된다.

이러한 성능 저하는 반정규화를 통해 일부 해결할 수 있다. 정규화가 테이블 간의 중복을 제거한다면, 반정규화는 일부 중복을 허용하거나, 서로 다른 테이블의 데이터를 통합한다.

## 2. 반정규화된 테이블의 성능저하 사례

상황과 조건에 따라서 정규화를 했을 때 성능저하가 발생하기도 하지만, 반대로 반정규화를 했을 때 성능저하가 발생하는 경우도 많이 볼 수 있었다.

솔직히 정규화랑 반정규화도 방금 배웠는데, 각각의 사례가 어떤 맥락에서 성능저하가 되는 건지까지는 잘 알기 힘들어서 일단 사례는 패스했다.

## 3. 결론?

일단은 정말 대부분의 상황에서는 정규화를 하는 것이 좋다. 정규화를 함으로써 생기는 부작용보다는 얻을 수 있는 이점이 훨~씬 많기 때문에 일단 문제가 생기더라도 정규화를 해보고 고민해보는 것이 좋다.

따라서, 데이터 모델링 시에는 1차적으로 제 3차 정규화의 과정까지는 거치고, 이후 일어나는 문제에 대해서 적절한 반정규화로 타협을 하는 것이 좋은 것 같다.

# 어려웠던 내용


## 부분키와 일반 컬럼의 구분

제 2정규화와 제 3정규화를 구분할 때 쓰이는 부분키와 일반 컬럼을 구분하는데 좀 어려움이 있었다.

부분키 → PK와 합쳤을 때 여전히 unique할 수 있는 속성

일반 컬럼 → PK와 합치더라도 unique하지 않은 속성

위와 같은 특성이 있기 때문에 해당 속성이 부분키가 될 수 있는지 잘 모르겠다면, 일단 PK와 합쳐보고 둘을 통해 모든 행을 unique하게 구분할 수 있는지를 보면 될 것 같다.

아래 예시를 보았을 때, 회원번호 컬럼(PK)과 프로그램 컬럼을 합치면 여전히 unique하기 때문에 부분키 쌉가능이다.

![](https://prod-files-secure.s3.us-west-2.amazonaws.com/5486ac02-837a-4340-b853-a8cd7b03f65f/cad618f6-e125-4976-8d2c-c65794720238/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2024-02-20_%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB_10.25.04.png)

# 궁금한 내용 / 부족한 내용


## Q. 입력/수정/삭제 성능과 조회 성능 간의 적절한 Trade-Off 지점은 어디일까?

일반적으로 정규화를 하면 입력/수정/삭제 성능이 향상, 조회 성능이 저하되는 것으로 알려져있다.

정규화를 했을 때 조회 성능이 저하되는 이유는, 정규화 자체가 테이블을 잘게 쪼개는 작업이기 때문에 이렇게 쪼개진 테이블들을 조회하려고 하면 다시 JOIN 연산을 통해 붙여야하기 때문이다.

그렇다면 과연 나는 몇 건의 데이터부터 정규화로 인한 조회 성능 저하를 걱정해야하는 것인가라는 궁금증이 들어 찾아보게 되었다.

A. GPT에게 물어본 결과, 일반적으로 정규화를 미리 진행하고 적절한 인덱싱 및 쿼리 최적화를 적용한다면, 적어도 수천에서 수백만 건의 데이터를 무리없이 처리할 수 있다고 한다.

이는 데이터 규모로서는 소규모~중규모에 속하는데, 즉 본인이 네이버/카카오와 같은 **정말 대규모 서비스를 관리하는 게 아니라면** 정규화의 부작용을 걱정하느라 정규화를 안 쓰는 건 바보짓인 것 같다는 생각이 들었다.

# 느낀점

정규화에 대해 전에 배웠었지만 정말 거의 다 까먹어서 솔직히 새로 공부하는 기분이었는데, 이번에는 두 번째 세 번째 봐서 그런가 이해가 너무 잘되서 약간 놀랐다 ㅋㅋㅋ

근데 생각해보니 나도 전에 DB 설계를 해봤을 때 데이터 일관성을 유지하기 위해서 나도 모르게 테이블을 분리하고 정규화를 했던 부분이 있었는데, 사람이 누구나 효율적으로 프로세스 혹은 코드를 짜려고 하면 뭔가 같은 방향으로 가게 되는 것 같다.

자꾸 반정규화를 배우지도 않았는데 정규화와 반정규화를 비교하는 내용이 나와서 살짝 당황했지만, 이후 8챕터에서 반정규화를 다시 학습한 다음에 7챕터에서의 비교 내용도 한 번 다시 보면 이해가 잘될 것 같아서, 그렇게 하려고 한다.